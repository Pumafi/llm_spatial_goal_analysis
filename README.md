# llm_spatial_goal_analysis
A very simple Mechanistic Interpretability project

# Studying Goal Representation in LLMs

## Introduction

I am a postodoctoral researcher in ML, and before that a computer science engineer by training. This blog post is to be seen as a way for me to get my hands dirty, and dive into Mechanistic Interpretability concretely for the first time after finishing my Technical AI Safety certification from Blue Dot.

## Executive Summary

> ❓ **Research Question**  
> This work does surface exploration about the following questions:
>
> 1. Do AI agents represents goal in a way that can be extracted ?
> 2. What can we learn about this goal representation ?


## Methodology

- The study focuses on **TODO**:


- **DATASET** were generated, simulating conversational contexts where XXX.
- **Methods** were employed:
  - **Principal Component Analysis (PCA)**
  - **Linear logistic probing**  
- **Metrics**

## Key Results

- XXX
- LIMITATION

Here’s the link to my git:
